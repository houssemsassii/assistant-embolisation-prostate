"""
Script d'ingestion des documents PDF pour le chatbot m√©dical RAG.
Construit un index FAISS √† partir des PDF fournis.

‚ö†Ô∏è USAGE M√âDICAL - Ce script traite des documents m√©dicaux valid√©s
pour cr√©er une base de connaissances stricte (pas d'hallucinations).
"""

import os
import sys
from pathlib import Path
from typing import List
import pickle

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from dotenv import load_dotenv

# Chargement des variables d'environnement
load_dotenv()

# ============================================
# CONFIGURATION
# ============================================

PDF_DIR = Path("data/pdfs")
VECTOR_STORE_DIR = Path("vector_store")
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "500"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "50"))

# Mod√®le d'embeddings fran√ßais (optimis√© pour le fran√ßais)
EMBEDDING_MODEL = "dangvantuan/sentence-camembert-large"


def load_pdfs(pdf_directory: Path) -> List:
    """
    Charge tous les fichiers PDF du r√©pertoire sp√©cifi√©.
    
    Args:
        pdf_directory: Chemin vers le dossier contenant les PDFs
        
    Returns:
        Liste de documents LangChain avec m√©tadonn√©es
    """
    if not pdf_directory.exists():
        print(f"‚ùå Erreur: Le dossier {pdf_directory} n'existe pas.")
        print(f"   Cr√©ez le dossier et placez-y vos PDFs sur l'embolisation de la prostate.")
        sys.exit(1)
    
    pdf_files = list(pdf_directory.glob("*.pdf"))
    
    if not pdf_files:
        print(f"‚ùå Erreur: Aucun fichier PDF trouv√© dans {pdf_directory}")
        sys.exit(1)
    
    print(f"üìÑ {len(pdf_files)} fichier(s) PDF trouv√©(s):")
    for pdf_file in pdf_files:
        print(f"   ‚Ä¢ {pdf_file.name}")
    
    all_documents = []
    
    for pdf_file in pdf_files:
        print(f"\nüìñ Chargement de: {pdf_file.name}")
        try:
            loader = PyPDFLoader(str(pdf_file))
            documents = loader.load()
            
            # Ajout de m√©tadonn√©es personnalis√©es
            for doc in documents:
                doc.metadata.update({
                    "source_file": pdf_file.name,
                    "procedure": "embolisation de la prostate",
                    "type": "document_patient",
                    "langue": "fran√ßais"
                })
            
            all_documents.extend(documents)
            print(f"   ‚úÖ {len(documents)} page(s) charg√©e(s)")
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur lors du chargement de {pdf_file.name}: {e}")
            continue
    
    if not all_documents:
        print("\n‚ùå Aucun document n'a pu √™tre charg√©.")
        sys.exit(1)
    
    print(f"\n‚úÖ Total: {len(all_documents)} page(s) charg√©e(s)")
    return all_documents


def split_documents(documents: List) -> List:
    """
    D√©coupe les documents en chunks de taille appropri√©e.
    
    Args:
        documents: Liste de documents LangChain
        
    Returns:
        Liste de chunks avec m√©tadonn√©es pr√©serv√©es
    """
    print(f"\n‚úÇÔ∏è  D√©coupage des documents en chunks...")
    print(f"   Taille de chunk: {CHUNK_SIZE} caract√®res")
    print(f"   Chevauchement: {CHUNK_OVERLAP} caract√®res")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        separators=["\n\n", "\n", ". ", " ", ""],
        add_start_index=True
    )
    
    chunks = text_splitter.split_documents(documents)
    
    print(f"   ‚úÖ {len(chunks)} chunk(s) cr√©√©(s)")
    
    # Affichage d'un exemple de chunk pour v√©rification
    if chunks:
        print(f"\nüìù Exemple de chunk (premier):")
        print(f"   Source: {chunks[0].metadata.get('source_file', 'N/A')}")
        print(f"   Contenu (100 premiers caract√®res): {chunks[0].page_content[:100]}...")
    
    return chunks


def create_embeddings():
    """
    Cr√©e le mod√®le d'embeddings fran√ßais.
    """
    print(f"\nüß† Initialisation du mod√®le d'embeddings...")
    print(f"   Mod√®le: {EMBEDDING_MODEL}")
    print(f"   ‚è≥ T√©l√©chargement en cours (peut prendre quelques minutes la premi√®re fois)...")
    
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    print(f"   ‚úÖ Mod√®le charg√© avec succ√®s")
    return embeddings


def build_vector_store(chunks: List, embeddings) -> FAISS:
    """
    Construit l'index FAISS √† partir des chunks.
    
    Args:
        chunks: Liste de chunks de documents
        embeddings: Mod√®le d'embeddings
        
    Returns:
        Vector store FAISS
    """
    print(f"\nüîç Construction de l'index vectoriel FAISS...")
    print(f"   ‚è≥ G√©n√©ration des embeddings pour {len(chunks)} chunks...")
    print(f"   (Cela peut prendre plusieurs minutes)")
    
    vector_store = FAISS.from_documents(chunks, embeddings)
    
    print(f"   ‚úÖ Index FAISS cr√©√© avec succ√®s")
    return vector_store


def save_vector_store(vector_store: FAISS):
    """
    Sauvegarde l'index FAISS sur disque.
    
    Args:
        vector_store: Vector store FAISS √† sauvegarder
    """
    print(f"\nüíæ Sauvegarde de l'index vectoriel...")
    
    VECTOR_STORE_DIR.mkdir(exist_ok=True)
    
    # Sauvegarde de l'index FAISS
    vector_store.save_local(str(VECTOR_STORE_DIR))
    
    print(f"   ‚úÖ Index sauvegard√© dans: {VECTOR_STORE_DIR}/")
    print(f"   Fichiers cr√©√©s:")
    for file in VECTOR_STORE_DIR.iterdir():
        print(f"      ‚Ä¢ {file.name}")


def main():
    """
    Pipeline principal d'ingestion.
    """
    print("=" * 70)
    print("üè• INGESTION DES DOCUMENTS M√âDICAUX")
    print("   Embolisation de la prostate - Base de connaissances RAG")
    print("=" * 70)
    
    # 1. Chargement des PDFs
    documents = load_pdfs(PDF_DIR)
    
    # 2. D√©coupage en chunks
    chunks = split_documents(documents)
    
    # 3. Cr√©ation des embeddings
    embeddings = create_embeddings()
    
    # 4. Construction de l'index vectoriel
    vector_store = build_vector_store(chunks, embeddings)
    
    # 5. Sauvegarde
    save_vector_store(vector_store)
    
    print("\n" + "=" * 70)
    print("‚úÖ INGESTION TERMIN√âE AVEC SUCC√àS")
    print("=" * 70)
    print(f"\nüìä Statistiques:")
    print(f"   ‚Ä¢ Documents trait√©s: {len(documents)} pages")
    print(f"   ‚Ä¢ Chunks cr√©√©s: {len(chunks)}")
    print(f"   ‚Ä¢ Index sauvegard√©: {VECTOR_STORE_DIR}/")
    print(f"\nüöÄ Vous pouvez maintenant lancer l'application:")
    print(f"   streamlit run app.py")
    print()


if __name__ == "__main__":
    main()
