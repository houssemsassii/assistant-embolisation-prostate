"""
Script d'ingestion des documents PDF pour le chatbot m√©dical RAG.
Construit un index FAISS √† partir des PDF fournis avec:
- Semantic chunking (d√©coupage s√©mantique)
- Pr√©paration pour hybrid retrieval (vector + keyword search)

‚ö†Ô∏è USAGE M√âDICAL - Ce script traite des documents m√©dicaux valid√©s
pour cr√©er une base de connaissances stricte (pas d'hallucinations).
"""

import os
import sys
from pathlib import Path
from typing import List
import pickle

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_experimental.text_splitter import SemanticChunker
from dotenv import load_dotenv

# Chargement des variables d'environnement
load_dotenv()

# ============================================
# CONFIGURATION
# ============================================

PDF_DIR = Path("data/pdfs")
VECTOR_STORE_DIR = Path("vector_store")

# Configuration pour semantic chunking
USE_SEMANTIC_CHUNKING = os.getenv("USE_SEMANTIC_CHUNKING", "true").lower() == "true"

# Fallback configuration si semantic chunking √©choue
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "500"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "100"))

# Mod√®le d'embeddings fran√ßais (optimis√© pour le fran√ßais)
EMBEDDING_MODEL = "dangvantuan/sentence-camembert-large"


def load_pdfs(pdf_directory: Path) -> List:
    """
    Charge tous les fichiers PDF du r√©pertoire sp√©cifi√©.
    
    Args:
        pdf_directory: Chemin vers le dossier contenant les PDFs
        
    Returns:
        Liste de documents LangChain avec m√©tadonn√©es
    """
    if not pdf_directory.exists():
        print(f"‚ùå Erreur: Le dossier {pdf_directory} n'existe pas.")
        print(f"   Cr√©ez le dossier et placez-y vos PDFs sur l'embolisation de la prostate.")
        sys.exit(1)
    
    pdf_files = list(pdf_directory.glob("*.pdf"))
    
    if not pdf_files:
        print(f"‚ùå Erreur: Aucun fichier PDF trouv√© dans {pdf_directory}")
        sys.exit(1)
    
    print(f"üìÑ {len(pdf_files)} fichier(s) PDF trouv√©(s):")
    for pdf_file in pdf_files:
        print(f"   ‚Ä¢ {pdf_file.name}")
    
    all_documents = []
    
    for pdf_file in pdf_files:
        print(f"\nüìñ Chargement de: {pdf_file.name}")
        try:
            loader = PyPDFLoader(str(pdf_file))
            documents = loader.load()
            
            # Ajout de m√©tadonn√©es personnalis√©es
            for doc in documents:
                doc.metadata.update({
                    "source_file": pdf_file.name,
                    "procedure": "embolisation de la prostate",
                    "type": "document_patient",
                    "langue": "fran√ßais"
                })
            
            all_documents.extend(documents)
            print(f"   ‚úÖ {len(documents)} page(s) charg√©e(s)")
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Erreur lors du chargement de {pdf_file.name}: {e}")
            continue
    
    if not all_documents:
        print("\n‚ùå Aucun document n'a pu √™tre charg√©.")
        sys.exit(1)
    
    print(f"\n‚úÖ Total: {len(all_documents)} page(s) charg√©e(s)")
    return all_documents


def split_documents_semantic(documents: List, embeddings) -> List:
    """
    D√©coupe les documents en utilisant le semantic chunking.
    Les chunks sont cr√©√©s en fonction de la similarit√© s√©mantique du contenu.
    
    Args:
        documents: Liste de documents LangChain
        embeddings: Mod√®le d'embeddings pour le semantic chunking
        
    Returns:
        Liste de chunks avec m√©tadonn√©es pr√©serv√©es
    """
    print(f"\n‚úÇÔ∏è  D√©coupage s√©mantique des documents...")
    print(f"   M√©thode: Semantic Chunking (bas√© sur similarit√© s√©mantique)")
    print(f"   ‚è≥ Analyse de la structure s√©mantique en cours...")
    
    try:
        # Semantic chunker avec percentile breakpoint
        text_splitter = SemanticChunker(
            embeddings,
            breakpoint_threshold_type="percentile",
            breakpoint_threshold_amount=80  # Seuil de similarit√©
        )
        
        chunks = text_splitter.split_documents(documents)
        
        print(f"   ‚úÖ {len(chunks)} chunk(s) s√©mantique(s) cr√©√©(s)")
        
        # Statistiques sur la taille des chunks
        chunk_sizes = [len(chunk.page_content) for chunk in chunks]
        if chunk_sizes:
            avg_size = sum(chunk_sizes) / len(chunk_sizes)
            min_size = min(chunk_sizes)
            max_size = max(chunk_sizes)
            print(f"   üìä Statistiques des chunks:")
            print(f"      - Taille moyenne: {int(avg_size)} caract√®res")
            print(f"      - Taille min: {min_size} caract√®res")
            print(f"      - Taille max: {max_size} caract√®res")
        
    except Exception as e:
        print(f"   ‚ö†Ô∏è Erreur lors du semantic chunking: {e}")
        print(f"   üîÑ Basculement vers RecursiveCharacterTextSplitter...")
        return split_documents_recursive(documents)
    
    # Affichage d'un exemple de chunk pour v√©rification
    if chunks:
        print(f"\nüìù Exemple de chunk s√©mantique (premier):")
        print(f"   Source: {chunks[0].metadata.get('source_file', 'N/A')}")
        print(f"   Contenu (100 premiers caract√®res): {chunks[0].page_content[:100]}...")
    
    return chunks


def split_documents_recursive(documents: List) -> List:
    """
    D√©coupe les documents avec RecursiveCharacterTextSplitter (fallback).
    
    Args:
        documents: Liste de documents LangChain
        
    Returns:
        Liste de chunks avec m√©tadonn√©es pr√©serv√©es
    """
    print(f"\n‚úÇÔ∏è  D√©coupage r√©cursif des documents...")
    print(f"   Taille de chunk: {CHUNK_SIZE} caract√®res")
    print(f"   Chevauchement: {CHUNK_OVERLAP} caract√®res")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        separators=["\n\n", "\n", ". ", "! ", "? ", ", ", " ", ""],
        add_start_index=True
    )
    
    chunks = text_splitter.split_documents(documents)
    
    print(f"   ‚úÖ {len(chunks)} chunk(s) cr√©√©(s)")
    
    # Affichage d'un exemple de chunk pour v√©rification
    if chunks:
        print(f"\nüìù Exemple de chunk (premier):")
        print(f"   Source: {chunks[0].metadata.get('source_file', 'N/A')}")
        print(f"   Contenu (100 premiers caract√®res): {chunks[0].page_content[:100]}...")
    
    return chunks


def create_embeddings():
    """
    Cr√©e le mod√®le d'embeddings fran√ßais.
    """
    print(f"\nüß† Initialisation du mod√®le d'embeddings...")
    print(f"   Mod√®le: {EMBEDDING_MODEL}")
    print(f"   ‚è≥ T√©l√©chargement en cours (peut prendre quelques minutes la premi√®re fois)...")
    
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    print(f"   ‚úÖ Mod√®le charg√© avec succ√®s")
    return embeddings


def build_vector_store(chunks: List, embeddings) -> FAISS:
    """
    Construit l'index FAISS √† partir des chunks.
    
    Args:
        chunks: Liste de chunks de documents
        embeddings: Mod√®le d'embeddings
        
    Returns:
        Vector store FAISS
    """
    print(f"\nüîç Construction de l'index vectoriel FAISS...")
    print(f"   ‚è≥ G√©n√©ration des embeddings pour {len(chunks)} chunks...")
    print(f"   (Cela peut prendre plusieurs minutes)")
    
    vector_store = FAISS.from_documents(chunks, embeddings)
    
    print(f"   ‚úÖ Index FAISS cr√©√© avec succ√®s")
    return vector_store


def save_vector_store(vector_store: FAISS, chunks: List):
    """
    Sauvegarde l'index FAISS et les donn√©es pour hybrid retrieval.
    
    Args:
        vector_store: Vector store FAISS √† sauvegarder
        chunks: Liste des chunks (pour BM25)
    """
    print(f"\nüíæ Sauvegarde de l'index vectoriel...")
    
    VECTOR_STORE_DIR.mkdir(exist_ok=True)
    
    # Sauvegarde de l'index FAISS
    vector_store.save_local(str(VECTOR_STORE_DIR))
    print(f"   ‚úÖ Index FAISS sauvegard√©")
    
    # Sauvegarde des chunks pour BM25 (hybrid retrieval)
    chunks_file = VECTOR_STORE_DIR / "chunks.pkl"
    with open(chunks_file, 'wb') as f:
        pickle.dump(chunks, f)
    print(f"   ‚úÖ Chunks sauvegard√©s pour hybrid retrieval")
    
    print(f"\n   üìÅ Fichiers cr√©√©s dans {VECTOR_STORE_DIR}/:")
    for file in sorted(VECTOR_STORE_DIR.iterdir()):
        size = file.stat().st_size
        if size < 1024:
            size_str = f"{size} B"
        elif size < 1024 * 1024:
            size_str = f"{size / 1024:.1f} KB"
        else:
            size_str = f"{size / (1024 * 1024):.1f} MB"
        print(f"      ‚Ä¢ {file.name} ({size_str})")


def main():
    """
    Pipeline principal d'ingestion.
    """
    print("=" * 70)
    print("üè• INGESTION DES DOCUMENTS M√âDICAUX")
    print("   Embolisation de la prostate - Base de connaissances RAG")
    print("   Features: Semantic Chunking + Hybrid Retrieval")
    print("=" * 70)
    
    # 1. Chargement des PDFs
    documents = load_pdfs(PDF_DIR)
    
    # 2. Cr√©ation des embeddings (n√©cessaire avant le semantic chunking)
    embeddings = create_embeddings()
    
    # 3. D√©coupage en chunks (s√©mantique ou r√©cursif)
    if USE_SEMANTIC_CHUNKING:
        chunks = split_documents_semantic(documents, embeddings)
    else:
        chunks = split_documents_recursive(documents)
    
    # 4. Construction de l'index vectoriel
    vector_store = build_vector_store(chunks, embeddings)
    
    # 5. Sauvegarde (FAISS + chunks pour BM25)
    save_vector_store(vector_store, chunks)
    
    print("\n" + "=" * 70)
    print("‚úÖ INGESTION TERMIN√âE AVEC SUCC√àS")
    print("=" * 70)
    print(f"\nüìä Statistiques:")
    print(f"   ‚Ä¢ Documents trait√©s: {len(documents)} pages")
    print(f"   ‚Ä¢ Chunks cr√©√©s: {len(chunks)}")
    print(f"   ‚Ä¢ M√©thode: {'Semantic Chunking' if USE_SEMANTIC_CHUNKING else 'Recursive Chunking'}")
    print(f"   ‚Ä¢ Index sauvegard√©: {VECTOR_STORE_DIR}/")
    print(f"\nüöÄ Vous pouvez maintenant lancer l'application:")
    print(f"   streamlit run app.py")
    print(f"\nüí° L'application utilisera:")
    print(f"   ‚Ä¢ Vector Search (FAISS) - recherche s√©mantique")
    print(f"   ‚Ä¢ Keyword Search (BM25) - recherche par mots-cl√©s")
    print(f"   ‚Ä¢ Hybrid Retrieval - combinaison des deux m√©thodes")
    print()


if __name__ == "__main__":
    main()
