"""
Script d'ingestion multi-sources pour le chatbot m√©dical RAG.
Construit un index FAISS √† partir de:
- PDFs organis√©s par maladie/proc√©dure (data/pdfs/maladie/*.pdf)
- Pages web du site laradiologiequisoigne.fr

‚ö†Ô∏è USAGE M√âDICAL - Ce script traite des documents m√©dicaux valid√©s
pour cr√©er une base de connaissances stricte (pas d'hallucinations).
"""

import os
import sys
from pathlib import Path
from typing import List, Dict
import pickle

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
from dotenv import load_dotenv

# Chargement des variables d'environnement
load_dotenv()

# ============================================
# CONFIGURATION
# ============================================

PDF_DIR = Path("data/pdfs")
VECTOR_STORE_DIR = Path("vector_store")

# Configuration pour chunking optimis√©
CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "500"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "100"))

# Mod√®le d'embeddings fran√ßais
EMBEDDING_MODEL = "dangvantuan/sentence-camembert-large"

# URLs des pages web √† scraper par maladie (URLs CORRIG√âES - Janvier 2026)
WEB_URLS = {
    # Pages maladies (10 pages)
    "arthrose_genou": "https://www.laradiologiequisoigne.fr/gonarthrose/",
    "epaule_gelee": "https://www.laradiologiequisoigne.fr/epaule-gelee/",
    "prostate": "https://www.laradiologiequisoigne.fr/hyperplasie-benigne-de-la-prostate-hbp/",
    "fibrome_uterin": "https://www.laradiologiequisoigne.fr/fibrome-uterin/",
    "varicocele": "https://www.laradiologiequisoigne.fr/la-varicocele/",
    "hemorroides": "https://www.laradiologiequisoigne.fr/les-hemorroides/",
    "douleurs_marche": "https://www.laradiologiequisoigne.fr/claudication-intermittente/",
    "grosse_jambe": "https://www.laradiologiequisoigne.fr/la-grosse-jambe-post-phlebitique/",
    "cancer": "https://www.laradiologiequisoigne.fr/le-cancer/",
    "douleurs_osseuses": "https://www.laradiologiequisoigne.fr/douleurs-osseuses-chroniques/",
    
    # Pages g√©n√©rales (4 pages)
    "accueil": "https://www.laradiologiequisoigne.fr/",
    "service": "https://www.laradiologiequisoigne.fr/service-radiologie-interventionnelle-hegp/",
    "radiologues": "https://www.laradiologiequisoigne.fr/radiologues-interventionnels-hegp/",
    "actualites": "https://www.laradiologiequisoigne.fr/actualites-service/",
}


def detect_procedure_from_folder(folder_name: str) -> str:
    """
    D√©tecte la proc√©dure/maladie depuis le nom du dossier.
    
    Args:
        folder_name: Nom du dossier (ex: "embolisation_prostate")
    
    Returns:
        Nom de la proc√©dure format√©
    """
    # Mapping des noms de dossiers possibles
    procedure_mapping = {
        "embolisation_prostate": "Embolisation de la prostate",
        "prostate": "Embolisation de la prostate",
        "embolisation_uterine": "Embolisation ut√©rine",
        "fibrome_uterin": "Fibrome ut√©rin",
        "pose_pac": "Pose de PAC",
        "pac": "Pose de PAC",
        "biopsie_scanner": "Biopsie sous scanner",
        "biopsie": "Biopsie sous scanner",
        "arthrose_genou": "Arthrose du genou (gonarthrose)",
        "gonarthrose": "Arthrose du genou (gonarthrose)",
        "epaule_gelee": "√âpaule gel√©e (capsulite r√©tractile)",
        "capsulite": "√âpaule gel√©e (capsulite r√©tractile)",
        "varicocele": "Varicoc√®le",
        "hemorroides": "H√©morro√Ødes",
        "douleurs_marche": "Douleurs √† la marche",
        "grosse_jambe": "Grosse jambe post-phl√©bite",
        "cancer": "Cancer",
        "douleurs_osseuses": "Douleurs osseuses",
    }
    
    folder_lower = folder_name.lower().replace(" ", "_").replace("-", "_")
    return procedure_mapping.get(folder_lower, folder_name.replace("_", " ").title())


def load_pdfs_recursive(base_directory: Path) -> List[Document]:
    """
    Charge tous les PDFs de mani√®re r√©cursive depuis les sous-dossiers.
    Chaque sous-dossier repr√©sente une maladie/proc√©dure.
    
    Args:
        base_directory: Chemin vers le dossier racine contenant les sous-dossiers
        
    Returns:
        Liste de documents LangChain avec m√©tadonn√©es enrichies
    """
    if not base_directory.exists():
        print(f"‚ùå Erreur: Le dossier {base_directory} n'existe pas.")
        print(f"   Structure attendue: data/pdfs/maladie/*.pdf")
        sys.exit(1)
    
    all_documents = []
    subdirs = [d for d in base_directory.iterdir() if d.is_dir()]
    
    if not subdirs:
        # Fallback: chercher PDFs directement dans le dossier racine
        print("‚ö†Ô∏è  Aucun sous-dossier trouv√©, recherche des PDFs √† la racine...")
        subdirs = [base_directory]
    
    print(f"\nüìÅ Dossiers d√©tect√©s: {len(subdirs)}")
    
    for subdir in subdirs:
        procedure_name = detect_procedure_from_folder(subdir.name) if subdir != base_directory else "Radiologie interventionnelle"
        pdf_files = list(subdir.glob("*.pdf"))
        
        if not pdf_files:
            print(f"   ‚Ä¢ {subdir.name}: Aucun PDF")
            continue
        
        print(f"\nüìÇ Proc√©dure: {procedure_name}")
        print(f"   Dossier: {subdir.name}")
        print(f"   Fichiers: {len(pdf_files)} PDF(s)")
        
        for pdf_file in pdf_files:
            print(f"   üìñ Chargement: {pdf_file.name}")
            try:
                loader = PyPDFLoader(str(pdf_file))
                documents = loader.load()
                
                # Ajout de m√©tadonn√©es enrichies
                for doc in documents:
                    doc.metadata.update({
                        "source_file": pdf_file.name,
                        "source_type": "pdf",
                        "procedure": procedure_name,
                        "folder": subdir.name,
                        "type": "document_patient",
                        "langue": "fran√ßais"
                    })
                
                all_documents.extend(documents)
                print(f"      ‚úÖ {len(documents)} page(s) charg√©e(s)")
                
            except Exception as e:
                print(f"      ‚ö†Ô∏è Erreur: {e}")
                continue
    
    if not all_documents:
        print("\n‚ö†Ô∏è Aucun document PDF n'a pu √™tre charg√©.")
        print("   Le syst√®me continuera avec le contenu web uniquement.")
    else:
        print(f"\n‚úÖ Total PDFs: {len(all_documents)} page(s) charg√©e(s)")
    
    return all_documents


def scrape_website() -> List[Document]:
    """
    Scrape toutes les pages pertinentes du site laradiologiequisoigne.fr
    
    Returns:
        Liste de documents web avec m√©tadonn√©es
    """
    print(f"\nüåê Scraping du site web: laradiologiequisoigne.fr")
    print(f"   Pages √† scraper: {len(WEB_URLS)} (10 maladies + 4 pages g√©n√©rales)")
    
    web_documents = []
    
    for procedure_key, url in WEB_URLS.items():
        print(f"\n   üîó {procedure_key}: {url}")
        try:
            loader = WebBaseLoader(url)
            docs = loader.load()
            
            # D√©tection de la proc√©dure
            procedure_name = detect_procedure_from_folder(procedure_key)
            
            for doc in docs:
                # Nettoyage du contenu web (enlever navigation, footer, etc.)
                content = doc.page_content
                
                # Ajout de m√©tadonn√©es
                doc.metadata.update({
                    "source_url": url,
                    "source_type": "web",
                    "source_name": "laradiologiequisoigne.fr",
                    "procedure": procedure_name,
                    "type": "information_generale",
                    "langue": "fran√ßais"
                })
            
            web_documents.extend(docs)
            print(f"      ‚úÖ Contenu r√©cup√©r√© ({len(docs[0].page_content)} caract√®res)")
            
        except Exception as e:
            print(f"      ‚ö†Ô∏è Erreur lors du scraping: {e}")
            continue
    
    if not web_documents:
        print("\n‚ö†Ô∏è Aucune page web n'a pu √™tre scrap√©e.")
        print("   Le syst√®me continuera avec les PDFs uniquement.")
    else:
        print(f"\n‚úÖ Total Web: {len(web_documents)} page(s) scrap√©e(s)")
    
    return web_documents


def split_documents(documents: List[Document]) -> List[Document]:
    """
    D√©coupe les documents avec RecursiveCharacterTextSplitter optimis√©.
    
    Args:
        documents: Liste de documents LangChain
        
    Returns:
        Liste de chunks avec m√©tadonn√©es pr√©serv√©es
    """
    print(f"\n‚úÇÔ∏è  D√©coupage intelligent des documents...")
    print(f"   Taille de chunk: {CHUNK_SIZE} caract√®res")
    print(f"   Chevauchement: {CHUNK_OVERLAP} caract√®res")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        separators=["\n\n", "\n", ". ", "! ", "? ", ", ", " ", ""],
        add_start_index=True
    )
    
    chunks = text_splitter.split_documents(documents)
    
    print(f"   ‚úÖ {len(chunks)} chunk(s) cr√©√©(s)")
    
    # Statistiques par source
    pdf_chunks = [c for c in chunks if c.metadata.get('source_type') == 'pdf']
    web_chunks = [c for c in chunks if c.metadata.get('source_type') == 'web']
    
    if pdf_chunks:
        print(f"      ‚Ä¢ Chunks PDFs: {len(pdf_chunks)}")
    if web_chunks:
        print(f"      ‚Ä¢ Chunks Web: {len(web_chunks)}")
    
    # Affichage d'exemples
    if chunks:
        print(f"\nüìù Exemples de chunks:")
        if pdf_chunks:
            print(f"   [PDF] {pdf_chunks[0].metadata.get('procedure', 'N/A')}")
            print(f"         Source: {pdf_chunks[0].metadata.get('source_file', 'N/A')}")
            print(f"         Extrait: {pdf_chunks[0].page_content[:80]}...")
        if web_chunks:
            print(f"   [WEB] {web_chunks[0].metadata.get('procedure', 'N/A')}")
            print(f"         URL: {web_chunks[0].metadata.get('source_url', 'N/A')}")
            print(f"         Extrait: {web_chunks[0].page_content[:80]}...")
    
    return chunks


def create_embeddings():
    """
    Cr√©e le mod√®le d'embeddings fran√ßais.
    """
    print(f"\nüß† Initialisation du mod√®le d'embeddings...")
    print(f"   Mod√®le: {EMBEDDING_MODEL}")
    print(f"   ‚è≥ T√©l√©chargement en cours (peut prendre quelques minutes la premi√®re fois)...")
    
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    print(f"   ‚úÖ Mod√®le charg√© avec succ√®s")
    return embeddings


def build_vector_store(chunks: List[Document], embeddings) -> FAISS:
    """
    Construit l'index FAISS √† partir des chunks.
    
    Args:
        chunks: Liste de chunks de documents
        embeddings: Mod√®le d'embeddings
        
    Returns:
        Vector store FAISS
    """
    print(f"\nüîç Construction de l'index vectoriel FAISS...")
    print(f"   ‚è≥ G√©n√©ration des embeddings pour {len(chunks)} chunks...")
    print(f"   (Cela peut prendre plusieurs minutes)")
    
    vector_store = FAISS.from_documents(chunks, embeddings)
    
    print(f"   ‚úÖ Index FAISS cr√©√© avec succ√®s")
    return vector_store


def save_vector_store(vector_store: FAISS, chunks: List[Document]):
    """
    Sauvegarde l'index FAISS et les donn√©es pour hybrid retrieval.
    
    Args:
        vector_store: Vector store FAISS √† sauvegarder
        chunks: Liste des chunks (pour BM25)
    """
    print(f"\nüíæ Sauvegarde de l'index vectoriel...")
    
    VECTOR_STORE_DIR.mkdir(exist_ok=True)
    
    # Sauvegarde de l'index FAISS
    vector_store.save_local(str(VECTOR_STORE_DIR))
    print(f"   ‚úÖ Index FAISS sauvegard√©")
    
    # Sauvegarde des chunks pour BM25 (hybrid retrieval)
    chunks_file = VECTOR_STORE_DIR / "chunks.pkl"
    with open(chunks_file, 'wb') as f:
        pickle.dump(chunks, f)
    print(f"   ‚úÖ Chunks sauvegard√©s pour hybrid retrieval")
    
    print(f"\n   üìÅ Fichiers cr√©√©s dans {VECTOR_STORE_DIR}/:")
    for file in sorted(VECTOR_STORE_DIR.iterdir()):
        size = file.stat().st_size
        if size < 1024:
            size_str = f"{size} B"
        elif size < 1024 * 1024:
            size_str = f"{size / 1024:.1f} KB"
        else:
            size_str = f"{size / (1024 * 1024):.1f} MB"
        print(f"      ‚Ä¢ {file.name} ({size_str})")


def main():
    """
    Pipeline principal d'ingestion multi-sources.
    """
    print("=" * 80)
    print("üè• INGESTION MULTI-SOURCES - RADIOLOGIE INTERVENTIONNELLE")
    print("   Sources: PDFs (par maladie) + Site web (laradiologiequisoigne.fr)")
    print("   Features: Intelligent Chunking + Hybrid Retrieval")
    print("=" * 80)
    
    all_documents = []
    
    # 1. Chargement des PDFs (par sous-dossier)
    print("\nüìö PHASE 1: Chargement des PDFs")
    pdf_documents = load_pdfs_recursive(PDF_DIR)
    all_documents.extend(pdf_documents)
    
    # 2. Scraping du site web
    print("\nüåê PHASE 2: Scraping du site web")
    web_documents = scrape_website()
    all_documents.extend(web_documents)
    
    if not all_documents:
        print("\n‚ùå ERREUR: Aucun document charg√© (ni PDF ni Web)")
        print("   V√©rifiez que:")
        print("   1. Les PDFs sont dans data/pdfs/maladie/*.pdf")
        print("   2. La connexion internet fonctionne pour le scraping")
        sys.exit(1)
    
    print(f"\n‚úÖ Total documents charg√©s: {len(all_documents)}")
    print(f"   ‚Ä¢ PDFs: {len(pdf_documents)}")
    print(f"   ‚Ä¢ Web: {len(web_documents)}")
    
    # 3. D√©coupage en chunks
    print("\n‚úÇÔ∏è  PHASE 3: D√©coupage des documents")
    chunks = split_documents(all_documents)
    
    # 4. Cr√©ation des embeddings
    print("\nüß† PHASE 4: Cr√©ation des embeddings")
    embeddings = create_embeddings()
    
    # 5. Construction de l'index vectoriel
    print("\nüîç PHASE 5: Construction de l'index FAISS")
    vector_store = build_vector_store(chunks, embeddings)
    
    # 6. Sauvegarde (FAISS + chunks pour BM25)
    print("\nüíæ PHASE 6: Sauvegarde")
    save_vector_store(vector_store, chunks)
    
    print("\n" + "=" * 80)
    print("‚úÖ INGESTION TERMIN√âE AVEC SUCC√àS")
    print("=" * 80)
    print(f"\nüìä Statistiques finales:")
    print(f"   ‚Ä¢ Documents sources: {len(all_documents)}")
    print(f"     - PDFs: {len(pdf_documents)} pages")
    print(f"     - Web: {len(web_documents)} pages")
    print(f"   ‚Ä¢ Chunks cr√©√©s: {len(chunks)}")
    
    # Statistiques par proc√©dure
    procedures = {}
    for chunk in chunks:
        proc = chunk.metadata.get('procedure', 'Inconnu')
        procedures[proc] = procedures.get(proc, 0) + 1
    
    print(f"   ‚Ä¢ Proc√©dures couvertes: {len(procedures)}")
    for proc, count in sorted(procedures.items(), key=lambda x: x[1], reverse=True):
        print(f"     - {proc}: {count} chunks")
    
    print(f"\n   ‚Ä¢ Index sauvegard√©: {VECTOR_STORE_DIR}/")
    print(f"\nüöÄ Vous pouvez maintenant lancer l'application:")
    print(f"   streamlit run app.py")
    print(f"\nüí° L'application utilisera:")
    print(f"   ‚Ä¢ Vector Search (FAISS) - recherche s√©mantique")
    print(f"   ‚Ä¢ Keyword Search (BM25) - recherche par mots-cl√©s")
    print(f"   ‚Ä¢ Hybrid Retrieval - combinaison optimale")
    print(f"   ‚Ä¢ Multi-proc√©dures - toutes les maladies couvertes")
    print()


if __name__ == "__main__":
    main()
